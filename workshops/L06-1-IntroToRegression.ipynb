{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"instructions\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h1>Instructions</h1>\n",
    "<p>Look for the <b>3 Your Turn</b> sections to complete the code and/or answer questions.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 - Introduction to Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The method of regression is one of the oldest and most widely used analytics methods. The goal of regression is to produce a model that represents the ‘best fit’ to some observed data. Typically the model is a function describing some type of curve (lines, parabolas, etc.) that is determined by a set of parameters (e.g., slope and intercept). “Best fit” means that there is an optimal set of parameters according to an evaluation criteria we choose.\n",
    "\n",
    "A regression model attempts to predict the value of one variable, known as the **dependent variable**, **response variable**, or **label**, using the values of other variables, known as **independent variables**, **explanatory variables**, or **features**. Single regression has one label used to predict one feature. Multiple regression uses two of more feature variables. \n",
    "\n",
    "Virtually all machine learning models, including some of the latest deep learning methods, are a form of regression. These methods often suffer from the same problems, including overfitting and mathematically unstable fitting methods. \n",
    "\n",
    "Linear regression is the foundational form of regression. To solve linear regression, normally the **method of least squares** is used.  Here, we are looking to find parameters of the function that minimizes the square of the error between the predictions, which are the function output and the observed target values.\n",
    "\n",
    "## History\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a3/Tobias_Mayer.jpg\" title=\"Tobias Mayer\" style=\"float:left; margin-right: 7px; height: 150px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e3/Pierre-Simon_Laplace.jpg\" title=\"Pierre-Simon Laplace\" style=\"float: right; margin-left: 10px; height: 150px;\">\n",
    "Regression is based on the method of least squares or the method of minimum mean square error. The ideas around least squares or averaging errors have occurred over nearly three centuries. The fist known publication of a 'Method of Averages' was by the German astronomer Tobias Mayer in 1750 (illustration top left). \n",
    "\n",
    "Laplace used a similar method which he published in 1788 (illustration top right).\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/03/Legendre.jpg\" style=\"height: 150px; float: left; margin-right: 10px;\" title=\"Caricature of Legendre - published method of least squares\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9b/Carl_Friedrich_Gauss.jpg\" style=\"height: 150px; float: right; margin-left: 10px;\" title=\"Carl Friedrich Gauss - early adopter of least squares\">The first publication of the method or least squares was by the French mathematician Adrien-Marie Legendre in 1805 (middle left). \n",
    "\n",
    "It is very likely that the German physicist and mathematician Carl Friedrich Gauss (middle right) developed the method of least squares as early as 1795, but did not publish the method until 1809, aside from a reference in a letter in 1799. Gauss never disputed Legendre's priority in publication. Legendre did not return the favor, and opposed any notion that Gauss had used the method earlier. \n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0b/Francis_Galton.jpg\" style=\"height: 150px; float: left; margin-right: 10px;\" title=\"Francis Galton - inventor of regression\">The first use of the term **regression** was by Francis Galton, a cousin of Charles Darwin, in 1886 (bottom left). Galton was interested in determining which traits of plants and animals, including humans, could be said to be inherited. \n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/be/Karl_Pearson%3B_Sir_Francis_Galton.jpg\" style=\"height: 350px; float: right; margin-left: 10px;\" title=\"Karl Pearson(left) and Francis Galton(right)\">\n",
    "While Galton invented a modern form regression, it fell to Karl Pearson to put regression and multiple regression on a firm mathematical footing. Pearson's 1898 publication proposed a method of regression as we understand it today (Pearson and Galton, bottom right). \n",
    "\n",
    "Many others have expanded the theory of regression in the 120 years since Pearson's paper. Notably, Joseph Berkson published the logistic regression method in 1944, one of the first classification algorithms. In recent times the interest in machine learning has lead to a rapid increase in the numbers and types of regression models.\n",
    "\n",
    "In this and subsequent sections, we will introduce and expand the concepts of linear regression and several methods associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "\n",
    "We will focus here on **linear models** which are foundational:\n",
    "- Derived with linear algebra\n",
    "- Basis of many machine learning models\n",
    "- Understanding linear models is basis for understanding behavior of many statistical and ML models\n",
    "- Basis of time series models\n",
    "\n",
    "### Linear model of a straight line\n",
    "\n",
    "Let's have a look at the simplest case of a regression model for a straight line. If we have one feature and one label, and there are some number of values pairs, ${x_i,y_i}$, we can define a line that best fits that data.  \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/ymxb.jpg\" alt=\"y_equals_mx_plus_b\" style=\"width: 450px;\"/>\n",
    "<center>**Single regression model**</center>\n",
    "\n",
    "$$where\\\\\n",
    "slope = m = \\frac{rise}{run} = \\frac{\\delta y}{\\delta x}\\\\\n",
    "and\\\\\n",
    "y = b\\ at\\ x = 0$$\n",
    "\n",
    "\n",
    "If we have a number of values pairs, ${x_i,y_i}$, we can write the equation for the line with the errors as:\n",
    "\n",
    "$$y_i = mx_i + b + \\epsilon_i \\\\\n",
    "where \\\\\n",
    "\\epsilon_i = error$$\n",
    "\n",
    "We can visualize these errors as shown in the figure below.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/LSRegression.jpg\" alt=\"LS Regression\" style=\"width: 450px;\"/>\n",
    "<center>**Example of Least Squares Regression**</center>\n",
    "\n",
    "We want to solve for $m$ and $b$ by minimizing the error, $\\epsilon_i$. We call this **least squares regression**.\n",
    "\n",
    "$$min \\Sigma_i \\epsilon^2 = min \\Sigma_i{ (y_i - (mx_i + b))^2}$$\n",
    "\n",
    "There are lots of computationally efficient algorithms for finding minimums of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first regression model\n",
    "\n",
    "Let's give regression a try. The code in the cell below computes data pairs along a straight line. Normally distributed noise is added to the data values. Run this code and examine the head of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "x_data = np.linspace(x_start, x_end, n_points)\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points)\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "# Or alternatively:\n",
    "# sim_data = pd.DataFrame(data=np.column_stack((x_data, y_data)), columns=['x','y'])\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can visualize these data by executing the code in the cell below. Notice that nearly all the points  fall on a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(sim_data['x'], sim_data['y'], 'ko')\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model\n",
    "Now, you are ready to build and evaluate the model using Python. Python has libraries that contain linear modeling capabilities.\n",
    "\n",
    "The first python library that is popular for linear regression is `Scikit-learn`.  Scikit-learn is a library that has many different types of machine learning algorithms, but here we concern ourselves with the linear regression capabilities.\n",
    "\n",
    "The second library is called `Stats-models`.  For those that have experience with the programming language, R, this library is the most similar because it provides easy statistical analysis of linear models that are fitted to the data.\n",
    "\n",
    "To start, we will show how to use the library `Scikit-learn` for linear regression. Model creation in Scikit-learn generally has two steps, declaring the model formulation and then fitting the model.\n",
    "\n",
    "In our case we only have one independent variable and one dependent variable. The code in the cell below does the following:  \n",
    "\n",
    "- Computes the Python model object, `linear_model`, using the simple linear regression.\n",
    "- Uses the model object to compute scores (predicted values) for the dependent variable `y`. In this case, we just use the data that was originally used to compute the model. In a more general case, you can use other data to make predictions from the model.\n",
    "- Computes the residuals of the model.\n",
    "\n",
    "Execute this code and examine the head of the data frame computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# First initialize the model.\n",
    "linear_model = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "x_input = sim_data['x'].values.reshape(n_points, 1)\n",
    "y_output = sim_data['y'].values.reshape(n_points, 1)\n",
    "linear_model.fit(x_input, y_output)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = linear_model.predict(x_input)\n",
    "\n",
    "# Plot output\n",
    "plt.scatter(x_input, y_output)\n",
    "plt.plot(x_input, y_pred, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')\n",
    "\n",
    "# Print model parameters\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept: {0:.5f}'.format(linear_model.intercept_[0]))\n",
    "print('Slope : {0:.5f}'.format(linear_model.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats Model Package\n",
    "Now we show how to fit a line with Stats Models package. The model declaration and fitting are in a different format as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=sim_data)\n",
    "# Alternatively:\n",
    "#ols_model = sm.OLS(y_output, sm.add_constant(x_input))\n",
    "\n",
    "results = ols_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept: {0:.5f}'.format(results.params.Intercept))\n",
    "print('Slope : {0:.5f}'.format(results.params.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted to pandas dataframe\n",
    "sim_data['predicted'] = y_pred\n",
    "# Add residuals to pandas dataframe\n",
    "sim_data['resids'] = y_output - y_pred\n",
    "\n",
    "# View head of data frame\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the coefficients?  (m = slope, and b = y-intercept)\n",
    "m = linear_model.coef_[0]\n",
    "b = linear_model.intercept_\n",
    "print('m = {}'.format(m[0]))\n",
    "print('b = {}'.format(b[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Parameters!\n",
    "\n",
    "It is very important to know how to interpret the parameters.  Here is an example for the above parameters. (Your results will be different because of the random seed used in creating the data.)\n",
    "\n",
    "**Y Intercept**:  When x is zero, y is -0.53...\n",
    "    \n",
    "**Slope**:  When we increase x by 1 then we expect that y will go up by 1.07...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary Statistics\n",
    "The code in the cell below is fairly voluminous, but straightforward. In summary, the code computes summary statistics and makes diagnostic plots for ordinary linear models.\n",
    "\n",
    "The Scikit-learn library is primarily made for fitting models and getting predictions. For linear regression, we are also interested in many diagnostic plots.  To get these plots easily, we can use a different library called `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=sim_data)\n",
    "# Alternatively:\n",
    "#ols_model = sm.OLS(y_output, sm.add_constant(x_input))\n",
    "\n",
    "results = ols_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slope : {}'.format(results.params))\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn 1\n",
    "Create a regression model from synthetic data with intercept of 0 and maximum value at ${x = 10, y = 10}$, and with a the error having a standard deviation of 5. Plot the result of your model. How does this slope and intercept of this model compare to the model from the data with a standard deviation of 1? \n",
    "\n",
    "**Hint:** You need need to add columns named `score` and `resids` to the data frame before you create the plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New Regression Model sd=5\n",
    "\n",
    "# Replace the ??? with the correct parameters\n",
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, ???\n",
    "y_start, y_end = 0, ???\n",
    "y_sd = ???\n",
    "\n",
    "# Generate data columns\n",
    "x_data = np.linspace(x_start, x_end, n_points)\n",
    "y_error = np.random.normal(loc=0, scale=???, size=???)\n",
    "y_data = np.linspace(y_start, y_end, n_points) + ???\n",
    "\n",
    "# Put data in dataframe\n",
    "reg_data_5 = pd.DataFrame({'x':???, 'y':???})\n",
    "\n",
    "reg_data_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the ??? with the correct parameters\n",
    "\n",
    "# Switch to Stats Model Package\n",
    "# Fit the model and get the linear model summaries/plots.\n",
    "x5_input = reg_data_5['x']\n",
    "y5_output = reg_data_5['y']\n",
    "ols5_model = sm.ols(formula=???, data = ???) # Use a simple formula and the data generated above\n",
    "\n",
    "results5 = ols5_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "results5.???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the ??? with the correct parameters\n",
    "\n",
    "# Additional Statistics\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean5_y = np.mean(y5_output)\n",
    "sst5 = np.sum((y5_output - mean5_y)**2)\n",
    "sse5 = sst5 - results5.ssr\n",
    "print('SSE: {}'.format(???))\n",
    "print('SST: {}'.format(???))\n",
    "print('SSR: {}'.format(results5.???))\n",
    "print('RMSE: {}'.format(np.sqrt(results5.???)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results5.???)\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y5_pred = ols5_model.fit().predict(???)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(results5.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x5_input, y5_output)\n",
    "plt.plot(x5_input, y5_pred, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the ??? with the correct parameters\n",
    "\n",
    "# Compare slope and intercept\n",
    "print('sd=1 Intercept, Slope : \\n{}'.format(???.params))\n",
    "print('\\nsd=5 Intercept, Slope : \\n{}'.format(???.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression Models\n",
    "\n",
    "Now that you have built a regression model, let's look at how you can quantitatively evaluate the performance of a regression model. The evaluation of regression models is based on measurements of the errors.\n",
    "\n",
    "If we want to know how well we did, we must consider what a good benchmark would be.  A common easy benchmark is to see if we can predict better than just the mean of the target variable.\n",
    "\n",
    "If we define the following terms:\n",
    "\n",
    "$\\bar{Y}$ - The mean of the target values.\n",
    "\n",
    "$Y_{i}$ - Target value of the i-th point.\n",
    "\n",
    "$\\hat{Y_{i}}$ - Predicted value of the i-th point.\n",
    "\n",
    "Then we can define the following error metrics:\n",
    "\n",
    "**Sum of Squared Residuals (SSR)**: This is the value that the method of least squares was able to minimize to with the best fit line.\n",
    "\n",
    "SSR = $\\sum \\left( Y_{i} - \\hat{Y_{i}} \\right)^{2}$\n",
    "\n",
    "**Sum of Squared Total (SST)**: This is a measure of how bad a prediction the target-mean would be.\n",
    "\n",
    "SST = $\\sum \\left( Y_{i} - \\bar{Y} \\right)^{2}$\n",
    "\n",
    "**Sum of Squared Error (SSE)**: This is a sum of a squared pointwise difference of the (SST - SSR).  You can also think of this as a measurement of how much better we are explaining the variation than the mean.\n",
    "\n",
    "SSE = $\\sum \\left( \\hat{Y_{i}} - \\bar{Y} \\right)^{2}$\n",
    "\n",
    "It is the case that **SST = SSR + SSE**.\n",
    "\n",
    "We also will be calculating something called the **Root-Mean-Squared-Error**, or the **RMSE**.  The reason we do this is because the Sum-squared-error (also known as the mean-squared error) is in units of x-squared.  To get a handle on understanding the error in units of 'x', we take the square root of the mean-squared-error , or the square root of the sum-square-error.  This is the RMSE.\n",
    "\n",
    "---------------\n",
    "\n",
    "The errors of a regression model can be visualized as shown in the figure below. \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/Errors.jpg\" alt=\"Regression_Errors\" style=\"width: 450px;\"/>\n",
    "\n",
    "<center>**Measuring errors for a regression model**\n",
    "$$Where\\\\\n",
    "Y = [y_1, y_2, \\ldots, y_n]\\\\\n",
    "and\\\\\n",
    "y_i = ith\\ data\\ value\\\\\n",
    "\\bar{Y} = mean(Y)\\\\\n",
    "\\\\\\hat{y_i} = regression\\ estimate\\ of\\ y_i\\\\\n",
    "SSE = sum\\ square\\ explained\\ = \\Sigma_i{(\\hat{y_i} - \\bar{Y})^2}\\\\\n",
    "SSR = sum\\ square\\ residual\\ = \\Sigma_i{(y_i - \\hat{y_i})^2}\\\\\n",
    "SST = sum\\ square\\ total\\ = \\Sigma_i(y_i - \\bar{Y})^2$$\n",
    "\n",
    "The goal of regression is to minimize the residual error, $SSR$. Specifically we wish to explain the maximum amount of the variance in the original data as possible with our model. We can quantify this idea with coefficient of determination also known as $R^2$.\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSR}{SST}\\\\\n",
    "so\\ as\\\\\n",
    "SSR \\rightarrow 0\\\\\n",
    "R^2 \\rightarrow 1$$\n",
    "\n",
    "In words, $R^2$ is the fraction of the variance of the original data explained by the model. A model that perfectly explains the data has $R^2 = 1$. A model which does not explain the data at all has $R^2 = 0$.\n",
    "\n",
    "However, there are two problems with $R^2$. </center>\n",
    " - $R^2$ is not bias adjusted for degrees of freedom.\n",
    " - More importantly, there is no adjustment for the number of model parameters. As the number of model parameters increases $SSR$ will generally decrease. Without an adjustment you will get a false sense of model performance.\n",
    " \n",
    "To addresses these related issues, we can use adjusted $R^2$.\n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{\\frac{SSR}{df_{SSR}}}{\\frac{SST}{df_{SST}}} = 1 - \\frac{var_{residual}}{var_{total}}\\\\\n",
    "where\\\\\n",
    "df_{SSR} = n - k = SSR\\ degrees\\ of\\ freedom\\\\\n",
    "df_{SST} = n - 1 = SST\\ degrees\\ of\\ freedom$$\n",
    "\n",
    "This gives $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} = 1 - (1 - R^2) \\frac{n - 1}{n - k}\\\\ \n",
    "where\\\\\n",
    "n = number\\ of\\ data\\ samples\\\\\n",
    "k = number\\ of\\ model\\ coefficients$$\n",
    "\n",
    "Or, we can rewrite $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} =  1.0 - \\frac{SSR}{SST}  \\frac{n - 1}{n - 1 - k}$$\n",
    "\n",
    "Another measure of regression performance is root mean square error or $RMSE$:\n",
    "\n",
    "$$RMSE = \\sqrt{ \\frac{\\Sigma^n_{i-1} (y_i - \\hat{y_i})^2}{n}} = \\frac{\\sqrt{SSR}}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 2\n",
    "\n",
    "Examine the performance metrics for the previous two regressions with sd=1 and sd=5. \n",
    "- How do SSE, SSR, SST, $R^2$, and RMSE compare?\n",
    "\n",
    "**Hint**: Use `results.rsquared` to find the $R^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "When performing regression with numeric variables you will almost **always scale the data**.  Scaling data is important not just for regression, but most other machine learning models. Some reasons to scale regression data include:\n",
    "\n",
    "- The intercept may be a long way from the actual data. With scaled features, the intercept is at the center of the distribution. \n",
    "- Scaling prevents features with a large numerical range from overwhelming features with small numerical values. Numerical range is not an indicator of feature importance!\n",
    "- Another important point (that applies more outside of linear regression) is that scaling data can also shift the data to be nearer to zero and spread out more evenly.  These type of data shifts allow certain algorithms (especially non-linear iterative algorithms to converge to a solution quicker).\n",
    "\n",
    "There are several possible approaches to scaling data:\n",
    " - Scale the features or independent variables. This is the most common practice.\n",
    " - Scale the label or dependent variable.\n",
    " - Scale both, which is another common practice.\n",
    " \n",
    "In this case, we will just scale the one feature. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scale function for a column in a pandas df\n",
    "def scale(col):\n",
    "    mean_col = np.mean(col)\n",
    "    sd_col = np.std(col)\n",
    "    std = (col - mean_col) / sd_col\n",
    "    return std\n",
    "#\n",
    "# scaled_x = (x - min(x))/(max(x) - min(x))\n",
    "#\n",
    "# Add scaled x to data frame\n",
    "sim_data['x_scale'] = scale(sim_data['x'])\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the new `x_scale` feature has some additional attributes. These attributes are used to scale new data on which you are making predictions. This model **will not work on unscaled** data. \n",
    "\n",
    "Run the code in the cell below to create and evaluate a regression model using the scaled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_scaled = sim_data['x_scale']\n",
    "ols_model = sm.ols(formula='y ~ x_scale', data = sim_data)\n",
    "\n",
    "results = ols_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Scaled: Intercept, Slope : {}'.format(results.params))\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Scaled: Intercept t-value, Slope t-value: {}'.format(results.tvalues))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print('Slope:')\n",
    "print(results.t_test([1,0]))\n",
    "print('Intercept:')\n",
    "print(results.t_test([0,1]))\n",
    "\n",
    "print('\\nScaled: SSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Scaled Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results and compare them to the results for the unscaled regression. Which performance statistics are the same and which are different?\n",
    "\n",
    "## Your Turn 3\n",
    "\n",
    "In the cell below use the data you created earlier (sd=5) to compute and evaluate a regression model using a scaled feature. \n",
    "\n",
    "How do the scaled values differ from the unscaled results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New scaled regression model, sd=5\n",
    "\n",
    "# Replace the ??? with the correct parameters\n",
    "# Get scaled x\n",
    "scaled_x5_input = scale(x5_input)\n",
    "reg_data_5['x_scaled'] = scaled_x5_input\n",
    "\n",
    "# Fit the model and get the linear model summaries/plots.\n",
    "ols5_model_scaled = sm.ols(formula=???, data = ???) # use the new scaled x in the formula\n",
    "\n",
    "results5_scaled = ols5_model_scaled.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slope : {}'.format(results5_scaled.???))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 entries of the data reg_data_5 dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ??? with the correct parameters\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Intercept t-value, Slope t-value: {}'.format(results5_scaled.???))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print('Slope:')\n",
    "print(results5_scaled.t_test([???]))\n",
    "print('Intercept:')\n",
    "print(results5_scaled.t_test([???]))\n",
    "\n",
    "print('\\nScaled SSE, SST, SSR, and RMSE:')\n",
    "mean5_scaled_y = np.mean(y5_output)\n",
    "sst5_scaled = np.sum((y5_output - mean5_scaled_y)**2)\n",
    "sse5_scaled = sst5_scaled - results5_scaled.ssr\n",
    "print('SSE: {}'.format(???))\n",
    "print('SST: {}'.format(???))\n",
    "print('SSR: {}'.format(results5_scaled.???))\n",
    "print('RMSE: {}'.format(np.sqrt(results5_scaled.???)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results5_scaled.???)\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y5_pred_scaled = ols5_model_scaled.fit().predict(scaled_x5_input)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(results5_scaled.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (Scaled)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(scaled_x5_input, y5_output)\n",
    "plt.plot(scaled_x5_input, y5_pred_scaled, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (Scaled)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Assumptions\n",
    "\n",
    "At this point we should discuss a few key assumptions of linear regression. Keep these points in mind whenever you used these models. \n",
    "\n",
    "- There is a **linear relationship** between dependent variable and the **coefficients** of the independent variables.\n",
    "- Measurement error is independent and random. Technically, we say that the error is **independently and identically distributed, or iid**.\n",
    "- Errors arise from the dependent variable only.\n",
    "- There is no multicolinearity. In other words, there is no significant correlation between the independent variables.\n",
    "- Residuals are **homoscedastic** (constant variance).  In other words, the errors are the same across all groups of independent variables. The opposite of homoscedastic is **heteroscedastic**, where there is systematic variation in the residuals with label values.\n",
    "\n",
    "The diagram below illustrates the iid errors for the dependent variable only.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/IndependentErrors.jpg\" style=\"height: 350px;\" alt=\"Independent errors\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homoscedastic and Heteroscedastic Errors\n",
    "\n",
    "Let's elaborate on some of the assumptions for the linear model. \n",
    "\n",
    "$$y_i = mx_i + b + \\epsilon_i \\\\\n",
    "where \\\\\n",
    "\\epsilon_i = N(0, \\sigma)$$\n",
    "\n",
    "In this model the errors, $\\epsilon_i$, do not depend on the dependent variable `y`. In this case we say the errors are **homoscedastic**.\n",
    "\n",
    "But what if:\n",
    "\n",
    "$$\\epsilon_i = N(0, f(x_i))\\\\\n",
    "such\\ as\\\\\n",
    "\\epsilon_i = N(0, e^{x_i})$$\n",
    "\n",
    "These errors are now **heteroscedastic**, with the errors dependent on `x` and hence not constant in `y`.\n",
    "\n",
    "Let's look at an example. In the code below the error increases linearly as `x` increases. Run this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parameters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 5\n",
    "\n",
    "# Generate data columns\n",
    "x_data = np.linspace(x_start, x_end, n_points)\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points)\n",
    "# Now add heteroscedasticity to y_error\n",
    "y_error = y_error * np.linspace(0, 10, n_points)\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error\n",
    "\n",
    "# Put data in dataframe\n",
    "het_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "het_data.head()\n",
    "\n",
    "#mod.het = lm(y ~ x, data = reg.data.het)\n",
    "#reg.data.het = reg.data.het\n",
    "#reg.data.het$score <- predict(mod.het, data = reg.data.het)\n",
    "#reg.data.het$resids <- reg.data.het$y - reg.data.het$score\n",
    "#plot.regression(reg.data.het, mod.het, k = 2)\n",
    "#summary(mod.het)\n",
    "#plot(mod.het)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit linear model and plot results/summaries\n",
    "het_linear = sm.ols(formula = 'y ~ x', data = het_data).fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(het_linear.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y5_pred_het = het_linear.predict(het_data['x'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(het_linear.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (Heteroscedastic Error)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(het_data['x'], het_data['y'])\n",
    "plt.plot(het_data['x'], y5_pred_het, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (Heteroscedastic Error)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary graphs:\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as statsmodels\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "\n",
    "# Residuals vs Fitted Values\n",
    "residuals = het_linear.resid\n",
    "fitted_vals = het_linear.predict(het_data['x'])\n",
    "plt.plot(fitted_vals, residuals, 'o')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "\n",
    "# Create Q-Q Normal Plot of the Residuals\n",
    "statsmodels.qqplot(residuals, stats.norm, fit=True, line='45')\n",
    "\n",
    "# Fitted Values vs. Square Root of the Standardized Residuals\n",
    "standardized_resid = (het_linear.resid - np.min(het_linear.resid)) / np.max(het_linear.resid)\n",
    "\n",
    "# Leverage Plot (Cook's Distance)\n",
    "influence_plot(het_linear)\n",
    "\n",
    "# Additional stats models plots\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = statsmodels.graphics.plot_regress_exog(het_linear, \"x\", fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about these results, which violate the homoscedastic error assumption:\n",
    "\n",
    "- The plot of residuals vs. the predicted value shows a systematic increase from left to right.\n",
    "- The Q-Q plot and the histogram show that the distribution of residuals has heavy tails and deviates from normal.\n",
    "- The plot of the standardized residuals shows an increase from right to left on the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Linear Regressions are Not Just for Straight Lines\n",
    "\n",
    "A linear model is linear in its coefficients, but that does not mean we are limited to straight lines, **a common misconception**.  In fact, a linear model need only be linear in its coefficients. A **non-comprehensive** list of functions which can be included in a linear model includes:\n",
    "\n",
    "- Polynomials, but beware of polynomials of degree 3 or above.\n",
    "- Splines and smoothing kernels.\n",
    "- Trigonometric functions.\n",
    "- Logarithmic and exponential functions.\n",
    "- Interaction terms, which are the product of feature values. For example, the two-way interaction of `var1` and `var2` is specified at `var1:var2`, or `var1*var2`. Adding a third variable, `var3` the three-way interaction, including all two-way interactions is modeled as `var1:var2:var3`. \n",
    "\n",
    "## Polynomial Curve\n",
    "To clarify these concepts, let's look at an example. The code in the cell below computes a curved line using a second order polynomial with coefficients `c1 and c2` and adds normally distributed noise.  Notice that the polynomial is defined by a linear sum of the components, defined by the coefficients. **Pay attention to the scaling of the features.** Run this code and have a look at the head of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_data_poly(x1, y1, x2, y2, c1=1.0, c2=0.5, n=50, sd=3):\n",
    "    # Create x-sequence\n",
    "    x_data = np.linspace(x1, x2, n)\n",
    "    # Create y-sequence\n",
    "    y_data = np.linspace(y1, y2, n)\n",
    "    # Create curved y-data\n",
    "    error = np.random.normal(loc=0, scale=sd, size=n)\n",
    "    y_curved = c1 * y_data + c2 * y_data**2 + error\n",
    "    # Scale x:\n",
    "    x_scaled = scale(x_data)\n",
    "    # Create pandas dataframe\n",
    "    df = pd.DataFrame({'x': x_scaled,\n",
    "                       'y': y_curved})\n",
    "    return(df)\n",
    "\n",
    "reg_data_poly = sim_data_poly(0, 0, 10, 10, n=50, sd=3)\n",
    "\n",
    "reg_data_poly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Polynomial Model\n",
    "Next, you will compute a linear polynomial model for these data. The code in the  cell below uses the `I()` function which literally **Interprets** the argument. In this case `I(x^2)` is interpreted as the second order polynomial term. Run this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod_poly = sm.ols(formula = 'y ~ x + I(x**2)', data = reg_data_poly).fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(mod_poly.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y5_pred_poly = mod_poly.predict(reg_data_poly['x'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(mod_poly.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (Polynomial)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(reg_data_poly['x'], reg_data_poly['y'])\n",
    "plt.plot(reg_data_poly['x'], y5_pred_poly, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (Polynomial)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "There is quite a bit of new information both plotted and in the tables. Let's step through what all this means.\n",
    "\n",
    "- The plot of the data and the regression line: look at this plot and try to decide if the fit is reasonably good.\n",
    "- The histogram of the residuals: do these residuals appear to be close to normally distributed?\n",
    "\n",
    "The statistics we have already discussed.\n",
    "- The report from the `summary` method.\n",
    "  - The model formula\n",
    "  - Summary statistics of the residuals\n",
    "  - For each model coefficient, 1) the value of the coefficient, 2) the standard error of the coefficient, 3) the t statistic for the coefficient, and 4) the p-value of the coefficient. The null hypothesis for the coefficient is that it is 0, and not contributing to the model.\n",
    "  - The standard error of the residuals, defined as:\n",
    "$$rse = \\frac{\\Sigma^N_i(y_i - \\hat{y_i})^2}{df} = \\frac{\\Sigma^N_i(y_i - \\hat{y_i})^2}{N - k}\\\\\n",
    "where\\\\\n",
    "k = number\\ of\\ model\\ parameters\n",
    "$$\n",
    "  - $R^2$ and $R^2_{adj}$.\n",
    "  - The F statistic and p-value for the model. The null hypothesis is that the model is not explaining the data, or that the distribution of residuals is the same as the distribution of the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Polynomial Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import statsmodels.api as statsmodels\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "\n",
    "# Residuals vs Fitted Values\n",
    "residuals = mod_poly.resid\n",
    "fitted_vals = mod_poly.predict(reg_data_poly['x'])\n",
    "plt.plot(fitted_vals, residuals, 'o')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "\n",
    "# Create Q-Q Normal Plot of the Residuals\n",
    "statsmodels.qqplot(residuals, stats.norm, fit=True, line='45')\n",
    "\n",
    "# Fitted Values vs. Square Root of the Standardized Residuals\n",
    "standardized_resid = (mod_poly.resid - np.min(mod_poly.resid)) / np.max(mod_poly.resid)\n",
    "\n",
    "# Leverage Plot (Cook's Distance)\n",
    "influence_plot(mod_poly)\n",
    "\n",
    "# Additional stats models plots\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = statsmodels.graphics.plot_regress_exog(mod_poly, \"x\", fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting More Results\n",
    "- A plot of fitted values (y in this case) vs. the residuals. Note the fitted smoothing regression line. Ideally, the distribution of residuals should not change with fitted values. \n",
    "- A Q-Q Normal plot of the residuals. Do these residuals appear to be close to Normally distributed?\n",
    "- A plot of fitted values vs. the square root of the standardized residuals. Note the fitted smoothing regression line. Ideally, the distribution of residuals should not change with fitted values and should be in the range $0 \\le \\sqrt{stdresid} \\le 1.5$ standard deviations. \n",
    "\n",
    "- A leverage plot showing Cooks distance. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage and Cook's Distance\n",
    "\n",
    "Up to now, we have only looked at regression models with normally distributed noise or errors. But, in the real world there are errors and outliers in data. These errors and outliers can have greater or lesser effect, depending on how extreme they are and their placement with respect to the other data. \n",
    "\n",
    "You can imagine a regression line as a lever. Outliers that occur near the ends of the lever will have a greater influence, all other factors being equal. \n",
    "\n",
    "One way to measure influence of a data point is Cook's distance, introduced by Dennis Cook in 1977. The influence for the `ith` data point can be computed as:\n",
    "\n",
    "$$D_i = \\frac{\\Sigma_{j=1}^n (\\hat{Y_j} - \\hat{Y_{j(i)}})^2}{n (p+1)\\hat{\\sigma^2}} \\\\\n",
    "where \\\\\n",
    "p = number\\ of\\ parameters\\\\\n",
    "n = number\\ of\\ data\\ points$$\n",
    "\n",
    "In effect, Cook's distance compares the difference between means with and without a given data point. Computing Cook's distance can be moderately computationally intensive for large data set. Typically, Cook's distance is measured in units of standard deviation.\n",
    "\n",
    "Let's make these concepts concrete with an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create new copy of original linear data\n",
    "outlier_data = sim_data[['x', 'y']].copy()\n",
    "# Add an outlier row\n",
    "outlier = pd.DataFrame([[0.0, 20.0]], columns = ['x', 'y'])\n",
    "outlier_data.append(outlier, ignore_index=True)\n",
    "\n",
    "# Scale data\n",
    "outlier_data['x_scaled'] = scale(outlier_data['x'])\n",
    "\n",
    "# Fit linear model\n",
    "outlier_linear = sm.ols(formula = 'y ~ x_scaled', data = outlier_data).fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(outlier_linear.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y_outlier = outlier_linear.predict(outlier_data['x_scaled'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(outlier_linear.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (With 1 Outlier)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(outlier_data['x_scaled'], outlier_data['y'])\n",
    "plt.plot(outlier_data['x_scaled'], y_outlier, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (With 1 Outlier)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Linear Regression Summary Plots\n",
    "# Summary graphs:\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as statsmodels\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "\n",
    "# Residuals vs Fitted Values\n",
    "residuals = outlier_linear.resid\n",
    "fitted_vals = outlier_linear.predict(outlier_data['x_scaled'])\n",
    "plt.plot(fitted_vals, residuals, 'o')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values (w/ 1 outlier)')\n",
    "\n",
    "# Create Q-Q Normal Plot of the Residuals\n",
    "statsmodels.qqplot(residuals, stats.norm, fit=True, line='45')\n",
    "\n",
    "# Fitted Values vs. Square Root of the Standardized Residuals\n",
    "standardized_resid = (outlier_linear.resid - np.min(outlier_linear.resid)) / np.max(outlier_linear.resid)\n",
    "\n",
    "# Leverage Plot (Cook's Distance)\n",
    "influence_plot(outlier_linear)\n",
    "\n",
    "# Additional stats models plots\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = statsmodels.graphics.plot_regress_exog(outlier_linear, \"x_scaled\", fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We looked at a lot of Regression concepts in this lesson so far:\n",
    "\n",
    "- Single regression equation of a line is $y_i = mx_i + b + \\epsilon_i $\n",
    "- Use the Python model object `linear_model.LinearRegression()` to initialize the model\n",
    "- Use the `fit` method to fit the model to the data\n",
    "- Use `predict` method to compute scores(predict values) for the dependent value $y$\n",
    "- Pull the model parameters $m$ (slope) with `coef` and $b$ (y-intercept) with `intercept`\n",
    "- The residuals are the difference between the y_output and the y_predicted\n",
    "\n",
    "- Use the `statsmodels` and `seaborn` libraries to get summary statistics and make diagnostic plots\n",
    "- `sm.ols(formula = 'y ~ x', data=sim_data)` and `fit()` method to get the results\n",
    "\n",
    "Evaluate the results from the OLS Regression Results with `summary` method\n",
    "- SSR the Sum of the Squared Residuals is the difference in error from the regression line (try to minimize)\n",
    "- SST the Sum of the Squared Total is the difference to the target mean\n",
    "- SSE the Sum of the Squared Error is the variation of the regression line to the mean\n",
    "- RMSE Root Mean Squared Error is the square root of the SSE\n",
    "- $R^2$ is the fraction of the variance of the original data explained by the model, want a number close to 1\n",
    "\n",
    "We created a function to scale data reconstructing the mean standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Your Turn</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
