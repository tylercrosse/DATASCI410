{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"instructions\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h1>Instructions</h1>\n",
    "<p>Look for the <b>3 Your Turn</b> sections to complete the code and/or answer questions.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 - Introduction to Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The method of regression is one of the oldest and most widely used analytics methods. The goal of regression is to produce a model that represents the ‘best fit’ to some observed data. Typically the model is a function describing some type of curve (lines, parabolas, etc.) that is determined by a set of parameters (e.g., slope and intercept). “Best fit” means that there is an optimal set of parameters according to an evaluation criteria we choose.\n",
    "\n",
    "A regression model attempts to predict the value of one variable, known as the **dependent variable**, **response variable** or **label**, using the values of other variables, known as **independent variables**, **explanatory variables** or **features**. Single regression has one label used to predict one feature. Multiple regression uses two of more feature variables. \n",
    "\n",
    "Virtually all machine learning models, including some of the latest deep learning methods, are a form of regression. These methods often suffer from the same problems, including overfitting and mathematically unstable fitting methods. \n",
    "\n",
    "Linear regression is the foundational form of regression. To solve linear regression, normally the **method of least squares** is used.  Here, we are looking to find parameters of the function that minimizes the square of the error between the predictions, which are the function output, and the observed target values.\n",
    "\n",
    "## History\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a3/Tobias_Mayer.jpg\" title=\"Tobias Mayer\" style=\"float:left; margin-right: 7px; height: 150px;\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e3/Pierre-Simon_Laplace.jpg\" title=\"Pierre-Simon Laplace\" style=\"float: right; margin-left: 10px; height: 150px;\">\n",
    "Regression is based on the method of least squares or the method of minimum mean square error. The ideas around least squares or averaging errors have occured over nearly three centuries. The fist known publication of a 'Method of Averages' was by the German astronomer Tobias Mayer in 1750. \n",
    "\n",
    "Lapace used a similar method which he published in 1788.\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/03/Legendre.jpg\" style=\"height: 150px; float: left; margin-right: 10px;\" title=\"Caricature of Legendre - published method of least squares\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9b/Carl_Friedrich_Gauss.jpg\" style=\"height: 150px; float: right; margin-left: 10px;\" title=\"Carl Friedrich Gauss - early adoptor of least squares\">The first publication of the method or least squares was by the French mathematician Adrien-Marie Legendre in 1805. \n",
    "\n",
    "It is very likely that the German physicist and mathematician Gauss developed the method of least squares as early as 1795, but did not publish the method until 1809, aside from a reference in a letter in 1799. Gauss never disputed Legendre's priority in publication. Legendre did not return the favor, and opposed any notion that Gauss had used the method earlier. \n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0b/Francis_Galton.jpg\" style=\"height: 150px; float: left; margin-right: 10px;\" title=\"Francis Galton - inventor of regression\">The first use of the term **regression** was by Francis Galton, a cousin of Charles Darwin, in 1886. Galton was interested in determining which traits of plants and animals, including humans, could be said to be inherited. \n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/be/Karl_Pearson%3B_Sir_Francis_Galton.jpg\" style=\"height: 350px; float: right; margin-left: 10px;\" title=\"Karl Pearson(left) and Francis Galton(right)\">\n",
    "While Galton invented a modern form regression, it fell to Karl Pearson to put regression and multiple regression on a firm mathematical footing. Pearson's 1898 publication proposed a method of regression as we understand it today. \n",
    "\n",
    "Many others have expanded the theory of regression in the 120 years since Pearson's paper. Notably, Joseph Berkson published the logistic regression method in 1944, one of the first classification algorithms. In recent times the interest in machine learning has lead to a rapid increase in the numbers and types of regression models.\n",
    "\n",
    "In this and subsequent sections, we will introduce and expand the concepts of linear regression and several methods associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "\n",
    "We will focus here on **linear models** which are foundational\n",
    "- Derived with linear algebra\n",
    "- Basis of many machine learning models\n",
    "- Understanding linear models is basis for understanding behavior of many statistical and ML models\n",
    "- Basis of time series models\n",
    "\n",
    "### Linear model of a straight line\n",
    "\n",
    "Let's have a look at the simplest case of a regression model for a straght line. If we have one feature and one label, there are some number of values pairs, ${x_i,y_i}$, we can define a line that best fits that data.  \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/ymxb.jpg\" alt=\"y_equals_mx_plus_b\" style=\"width: 450px;\"/>\n",
    "<center>**Single regression model**</center>\n",
    "\n",
    "$$where\\\\\n",
    "slope = m = \\frac{rise}{run} = \\frac{\\delta y}{\\delta x}\\\\\n",
    "and\\\\\n",
    "y = b\\ at\\ x = 0$$\n",
    "\n",
    "\n",
    "If we have a number of values pairs, ${x_i,y_i}$, we can write the equation for the line with the errors as:\n",
    "\n",
    "$$y_i = mx_i + b + \\epsilon_i \\\\\n",
    "where \\\\\n",
    "\\epsilon_i = error$$\n",
    "\n",
    "We can visualize these errors as shown in the figure below.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/LSRegression.jpg\" alt=\"LS Regression\" style=\"width: 450px;\"/>\n",
    "<center>**Example of Least Squares Regression**</center>\n",
    "\n",
    "We want to solve for $m$ and $b$ by minimizing the error, $\\epsilon_i$. We call this **least squares regression**.\n",
    "\n",
    "$$min \\Sigma_i \\epsilon^2 = min \\Sigma_i{ (y_i - (mx_i + b))^2}$$\n",
    "\n",
    "There are lots of computationally efficient algorithms for finding minimums of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first regression model\n",
    "\n",
    "Let's give regression a try. The code in the cell below computes data pairs along a straight line. Normally distributed noise is added to the data values. Run this code and examine the head of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, 10\n",
    "y_start, y_end = 0, 10\n",
    "y_sd = 1\n",
    "\n",
    "# Generate data columns\n",
    "x_data = np.linspace(x_start, x_end, n_points)\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points)\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error\n",
    "\n",
    "# Put data in dataframe\n",
    "sim_data = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "# Or alternatively:\n",
    "# sim_data = pd.DataFrame(data=np.column_stack((x_data, y_data)), columns=['x','y'])\n",
    "\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can visualize these data by exectuting the code in the cell below. Notice that the points nearly fall on a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(sim_data['x'], sim_data['y'], 'ko') # ko = black points\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model\n",
    "Now, you are ready to build and evaluate the model using Python. Python has libraries that contain linear modeling capabilities.\n",
    "\n",
    "The first python library that is popular for linear regression is `Scikit-learn`.  Scikit-learn is a library that has many different types of machine learning algorithms, but here we concern ourselves with the linear regression capabilities.\n",
    "\n",
    "The second library is called `Stats-models`.  For those that have experience with the programming language, R, this library is the most similar because it provides easy statistical analysis of linear models that are fitted to the data.\n",
    "\n",
    "To start, we will show how to use the library `Scikit-learn` for linear regression. Model creation in Scikit-learn generally has two steps, declaring the model formulation and then fitting the model.\n",
    "\n",
    "In our case we only have one independent variable and one dependent variable. The code in the cell below does the following:  \n",
    "\n",
    "- Compute the Python model object, `linear_model`, using the simple linear regression.\n",
    "- Use the model object to compute scores (predicted values) for the dependent variable `y`. In this case, we just use the data that was originally used to compute the model. In a more general case, you can use other data to make predictions from the model.\n",
    "- The residuals of the model are computed.\n",
    "\n",
    "Execute this code and examine the head of the data frame computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard template\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# generate random data-set\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 + 3 * x + np.random.rand(100, 1)\n",
    "\n",
    "# sckit-learn implementation\n",
    "\n",
    "# Model initialization\n",
    "regression_model = LinearRegression()\n",
    "# Fit the data(train the model)\n",
    "regression_model.fit(x, y)\n",
    "# Predict\n",
    "y_predicted = regression_model.predict(x)\n",
    "\n",
    "# model evaluation\n",
    "rmse = mean_squared_error(y, y_predicted)\n",
    "r2 = r2_score(y, y_predicted)\n",
    "\n",
    "# printing values\n",
    "print('Slope:' ,regression_model.coef_)\n",
    "print('Intercept:', regression_model.intercept_)\n",
    "print('Root mean squared error: ', rmse)\n",
    "print('R2 score: ', r2)\n",
    "\n",
    "# plotting values\n",
    "\n",
    "# data points\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# predicted values\n",
    "plt.plot(x, y_predicted, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.residplot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template on our dataset\n",
    "\n",
    "from sklearn import linear_model\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# First initialize the model.\n",
    "linear_model = linear_model.LinearRegression()\n",
    "\n",
    "# define x and y\n",
    "x_input = sim_data['x'].values.reshape(n_points, 1)\n",
    "y_output = sim_data['y'].values.reshape(n_points, 1)\n",
    "\n",
    "# fit the model to the existing data with lm.fit(x,y)\n",
    "linear_model.fit(x_input, y_output)\n",
    "\n",
    "# Use the fit model to make predictions\n",
    "y_pred = linear_model.predict(x_input)\n",
    "\n",
    "# Plot output\n",
    "plt.scatter(x_input, y_output)\n",
    "plt.plot(x_input, y_pred, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')\n",
    "\n",
    "# Print model parameters by viewing attributes intercept and coef\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept: {0:.5f}'.format(linear_model.intercept_[0]))\n",
    "print('Slope : {0:.5f}'.format(linear_model.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats Model Package\n",
    "Now we show how to fit a line with Stats Models package. The model declaration and fitting are in a different format as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# ols = ordinary least squares\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=sim_data)\n",
    "# Alternatively:\n",
    "#ols_model = sm.OLS(y_output, sm.add_constant(x_input))\n",
    "\n",
    "# fit the model\n",
    "results = ols_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept: {0:.5f}'.format(results.params.Intercept))\n",
    "print('Slope : {0:.5f}'.format(results.params.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted to pandas dataframe\n",
    "sim_data['predicted'] = y_pred\n",
    "# Add residuals to pandas dataframe\n",
    "sim_data['resids'] = y_output - y_pred\n",
    "\n",
    "# View head of data frame\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the coefficients?  (m = slope, and b = y-intercept)\n",
    "m = linear_model.coef_[0]\n",
    "b = linear_model.intercept_\n",
    "print('m = {}'.format(m[0]))\n",
    "print('b = {}'.format(b[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Parameters!\n",
    "\n",
    "It is very important to know how to interpret the parameters.  Here is an example for the above parameters. (Your results will be different because of the random seed in creating the data.)\n",
    "\n",
    "**Y Intercept**:  When x is zero, y is -0.53...\n",
    "    \n",
    "**Slope**:  When we increase x by 1 then we expect that y will go up by 1.07...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary Statistics\n",
    "The code in the cell below is fairly volumonous, but straight forward. In summary, the code computes summary statistics and makes diagnostic plots for ordinary linear models.\n",
    "\n",
    "The Scikit-learn library is primarily made for fitting models and getting predictions. For linear regression, we are also interested in many diagnostic plots.  To get these plots easily, we can use a different library called `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=sim_data)\n",
    "# Alternatively:\n",
    "#ols_model = sm.OLS(y_output, sm.add_constant(x_input))\n",
    "\n",
    "results = ols_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slope : {}'.format(results.params))\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn 1\n",
    "Create a regression model from synthetic data with intercept of 0 and maximum value at ${x = 10, y = 10}$, and with a the error having a standard deviation of 5. Plot the result of your model. How does this slope and intercept of this model compare to the model from the data with a standard deviation of 1? \n",
    "\n",
    "**Hint:** You need need to add columns named `score` and `resids` to the data frame before you create the plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New Regression Model sd=5\n",
    "\n",
    "# Replace the ??? with the correct parameters\n",
    "# Paramters of generated data\n",
    "n_points = 50\n",
    "x_start, x_end = 0, ??\n",
    "y_start, y_end = 0, ??\n",
    "y_sd = ??\n",
    "\n",
    "# Generate data columns\n",
    "x_data = np.linspace(x_start, x_end, n_points)\n",
    "y_error = np.random.normal(loc=0, scale=y_sd, size=n_points)\n",
    "y_data = np.linspace(y_start, y_end, n_points) + y_error\n",
    "\n",
    "# Put data in dataframe\n",
    "reg_data_5 = pd.DataFrame({'x':x_data, 'y':y_data})\n",
    "\n",
    "reg_data_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the ??? with the correct parameters\n",
    "# Fit the model and get the linear model summaries/plots.\n",
    "x5_input = reg_data_5[??]\n",
    "y5_output = reg_data_5[??]\n",
    "ols5_model = sm.ols(formula='y ~ x', data = reg_data_5)\n",
    "\n",
    "results5 = ols5_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "results5.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the ??? with the correct parameters\n",
    "# Additional Statistics\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean5_y = np.mean(y5_output)\n",
    "sst5 = np.sum((y5_output - mean5_y)**2)\n",
    "sse5 = sst5 - results5.ssr\n",
    "print('SSE: {}'.format(??))\n",
    "print('SST: {}'.format(??))\n",
    "print('SSR: {}'.format(results5.??))\n",
    "print('RMSE: {}'.format(np.sqrt(results5.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results5.summary())\n",
    "\n",
    "# Plot a histogram of the residuals, using x to predict y\n",
    "y5_pred = ols5_model.fit().predict(x5_input)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(results5.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x5_input, y5_output)\n",
    "plt.plot(x5_input, y5_pred, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the ??? with the correct parameters\n",
    "\n",
    "# Compare slope and intercept\n",
    "print('sd=1 Intercept, Slope : \\n{}'.format(results.params))\n",
    "print('\\nsd=5 Intercept, Slope : \\n{}'.format(results5.params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of regression models\n",
    "\n",
    "Now that you have built a regression model, let's look at how you can quantitatively evaluate the performance of a regression model. The evaluation of regression models is based on measurements of the errors.\n",
    "\n",
    "If we want to know how well we did, we must consider what a good benchmark would be.  A common easy benchmark is to see if we can predict better than just the mean of the target variable.\n",
    "\n",
    "If we define the following terms:\n",
    "\n",
    "$\\bar{Y}$ - The mean of the target values.\n",
    "\n",
    "$Y_{i}$ - Target value of the i-th point.\n",
    "\n",
    "$\\hat{Y_{i}}$ - Predicted value of the i-th point.\n",
    "\n",
    "Then we can defind the following error metrics:\n",
    "\n",
    "**Sum of Squared Residuals (SSR)**: This is the value that the method of least squares was able to minimize to with the best fit line.\n",
    "\n",
    "SSR = $\\sum \\left( Y_{i} - \\hat{Y_{i}} \\right)^{2}$\n",
    "\n",
    "**Sum of Squared Total (SST)**: This is a measure of how bad a prediction the target-mean would be.\n",
    "\n",
    "SST = $\\sum \\left( Y_{i} - \\bar{Y} \\right)^{2}$\n",
    "\n",
    "**Sum of Squared Error (SSE)**: This is a sum of a squared pointwise difference of the (SST - SSR).  You can also think of this as a measurement of how much better we are explaining the variation than the mean.\n",
    "\n",
    "SSE = $\\sum \\left( \\hat{Y_{i}} - \\bar{Y} \\right)^{2}$\n",
    "\n",
    "It is the case that **SST = SSR + SSE**.\n",
    "\n",
    "We also will be calculating something called the **Root-Mean-Squared-Error**, or the **RMSE**.  The reason we do this is because the Sum-squared-error (also known as the mean-squared error) is in units of x-squared.  To get a handle on understanding the error in units of 'x', we take the square root of the mean-squared-error , or the square root of the sum-square-error.  This is the RMSE.\n",
    "\n",
    "---------------\n",
    "\n",
    "The errors of a regression model can be visualized as shown in the figure below. \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/Errors.jpg\" alt=\"Regression_Errors\" style=\"width: 450px;\"/>\n",
    "\n",
    "<center>**Measuring errors for a regression model**\n",
    "$$Where\\\\\n",
    "Y = [y_1, y_2, \\ldots, y_n]\\\\\n",
    "and\\\\\n",
    "y_i = ith\\ data\\ value\\\\\n",
    "\\bar{Y} = mean(Y)\\\\\n",
    "\\\\\\hat{y_i} = regression\\ estimate\\ of\\ y_i\\\\\n",
    "SSE = sum\\ square\\ explained\\ = \\Sigma_i{(\\hat{y_i} - \\bar{Y})^2}\\\\\n",
    "SSR = sum\\ square\\ residual\\ = \\Sigma_i{(y_i - \\hat{y_i})^2}\\\\\n",
    "SST = sum\\ square\\ total\\ = \\Sigma_i(y_i - \\bar{Y})^2$$\n",
    "\n",
    "The goal of regression is to minimize the residual error, $SSR$. Specifically we wish to explain the maximum amount of the variance in the original data as possible with our model. We can quantify this idea with coefficient of determination also known as $R^2$.\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSR}{SST}\\\\\n",
    "so\\ as\\\\\n",
    "SSR \\rightarrow 0\\\\\n",
    "R^2 \\rightarrow 1$$\n",
    "\n",
    "In words, $R^2$ is the fraction of the variance of the original data explained by the model. A model that perfectly explains the data has $R^2 = 1$. A model which does not explain the data at all has $R^2 = 0$.\n",
    "\n",
    "However, there are two problems with $R^2$. </center>\n",
    " - $R^2$ is not bias adjusted for degrees of freedom.\n",
    " - More importantly, there is no adjustment for the number of model parameters. As the number of model parameters increases $SSR$ will generally decrease. Without an adjustment you will get a false sense of model performance.\n",
    " \n",
    "To addresses these related issues, we can use adjusted $R^2$.\n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{\\frac{SSR}{df_{SSR}}}{\\frac{SST}{df_{SST}}} = 1 - \\frac{var_{residual}}{var_{total}}\\\\\n",
    "where\\\\\n",
    "df_{SSR} = n - k = SSR\\ degrees\\ of\\ freedom\\\\\n",
    "df_{SST} = n - 1 = SST\\ degrees\\ of\\ freedom$$\n",
    "\n",
    "This gives $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} = 1 - (1 - R^2) \\frac{n - 1}{n - k}\\\\ \n",
    "where\\\\\n",
    "n = number\\ of\\ data\\ samples\\\\\n",
    "k = number\\ of\\ model\\ coefficients$$\n",
    "\n",
    "Or, we can rewrite $R^2_{adj}$ as:\n",
    "\n",
    "$$R^2_{adj} =  1.0 - \\frac{SSR}{SST}  \\frac{n - 1}{n - 1 - k}$$\n",
    "\n",
    "Another measure of regression performance is root mean square error or $RMSE$:\n",
    "\n",
    "$$RMSE = \\sqrt{ \\frac{\\Sigma^n_{i-1} (y_i - \\hat{y_i})^2}{n}} = \\frac{\\sqrt{SSR}}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 2\n",
    "\n",
    "Examine the performance metrics for the previous two regressions with sd=1 and sd=5. \n",
    "- How do SSE, SSR, SST, $R^2$, and RMSE compare?\n",
    "\n",
    "**Hint**: Use `results.rsquared` to find the $R^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill in ?? \n",
    "print('      sd1                sd5')\n",
    "print('SSE: ', sse, sse5)\n",
    "print('SSR: ', results.ssr, results5.ssr)\n",
    "print('SST: ', sst, sst5)\n",
    "print('Rsq: ', results.??,results5.??)\n",
    "print('RMSE:',np.sqrt(results.mse_model), np.sqrt(results5.mse_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage and Cook's Distance\n",
    "\n",
    "Up to now, we have only looked at regression models with Normally distributed noise or errors. But, in the real world there are errors and outliers in data. These errors and outliers can have greater or lesser effect, depending on how extreme they are and their placement with respect to the other data. \n",
    "\n",
    "You can imagine a regression line as a lever. Outliers that occur near the ends of the lever will have a greater influence all other factors being equal. \n",
    "\n",
    "One way to measure influence of a data point is Cook's distance, introduced by Dennis Cook in 1977. The influence for the `ith` data point can be computed as:\n",
    "\n",
    "$$D_i = \\frac{\\Sigma_{j=1}^n (\\hat{Y_j} - \\hat{Y_{j(i)}})^2}{n (p+1)\\hat{\\sigma^2}} \\\\\n",
    "where \\\\\n",
    "p = number\\ of\\ parameters\\\\\n",
    "n = number\\ of\\ data\\ points$$\n",
    "\n",
    "In effect, Cook's distance compares the difference between means with and without a given data point. Computing Cook's distance can be moderately computationally intensive for large data set. Typically, Cook's distance is measured in units of standard deviation.\n",
    "\n",
    "Let's make these concepts concrete with an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scale function for a column in a pandas df\n",
    "def scale(col):\n",
    "    mean_col = np.mean(col)\n",
    "    sd_col = np.std(col)\n",
    "    std = (col - mean_col) / sd_col\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create new copy of original linear data\n",
    "outlier_data = sim_data[['x', 'y']].copy()\n",
    "# Add an outlier row\n",
    "outlier = pd.DataFrame([[0.0, 20.0]], columns = ['x', 'y'])\n",
    "outlier_data.append(outlier, ignore_index=True)\n",
    "\n",
    "# Scale data\n",
    "outlier_data['x_scaled'] = scale(outlier_data['x'])\n",
    "\n",
    "# Fit linear model\n",
    "outlier_linear = sm.ols(formula = 'y ~ x_scaled', data = outlier_data).fit()\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(outlier_linear.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "y_outlier = outlier_linear.predict(outlier_data['x_scaled'])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(outlier_linear.resid, hist=True)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram (With 1 Outlier)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(outlier_data['x_scaled'], outlier_data['y'])\n",
    "plt.plot(outlier_data['x_scaled'], y_outlier, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y (With 1 Outlier)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Linear Regression Summary Plots\n",
    "# Summary graphs:\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as statsmodels\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "\n",
    "# Residuals vs Fitted Values\n",
    "residuals = outlier_linear.resid # outlier_linear = name of linear model on our dataset\n",
    "fitted_vals = outlier_linear.predict(outlier_data['x_scaled']) # making predictions from our fit model\n",
    "plt.plot(fitted_vals, residuals, 'o') # plotting predictions from fit model vs residuals\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Fitted Values (w/ 1 outlier)')\n",
    "\n",
    "# Create Q-Q Normal Plot of the Residuals\n",
    "statsmodels.qqplot(residuals, stats.norm, fit=True, line='45')\n",
    "\n",
    "# Fitted Values vs. Square Root of the Standardized Residuals\n",
    "standardized_resid = (outlier_linear.resid - np.min(outlier_linear.resid)) / np.max(outlier_linear.resid)\n",
    "\n",
    "# Leverage Plot (Cook's Distance)\n",
    "influence_plot(outlier_linear)  # from statsmodels.graphics.regressionplots import *\n",
    "\n",
    "# Additional stats models plots\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = statsmodels.graphics.plot_regress_exog(outlier_linear, \"x_scaled\", fig=fig)\n",
    "\n",
    "'''\n",
    "Plot regression results against one regressor.\n",
    "\n",
    "This plots four graphs in a 2 by 2 figure: ‘endog versus exog’, ‘residuals versus exog’, ‘fitted versus exog’ and ‘fitted plus residual versus exog’\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We looked at a lot of Regression concepts in this lesson so far:\n",
    "\n",
    "- Single regression equation of a line is $y_i = mx_i + b + \\epsilon_i $\n",
    "- Use the Python model object `linear_model.LinearRegression()` to initialize the model\n",
    "- Use the `fit` method to fit the model to the data\n",
    "- Use `predict` method to compute scores(predict values) for the dependent value $y$\n",
    "- Pull the model parameters $m$ (slope) with `coef` and $b$ (y-intercept) with `intercept`\n",
    "- The residuals are the difference between the y_output and the y_predicted\n",
    "\n",
    "- Use the `statsmodels` and `seaborn` libraries to get summary statistics and make diagnostic plots.\n",
    "- `sm.ols(formula = 'y ~ x', data=sim_data)` and `fit()` method to get the results\n",
    "\n",
    "Evaluate the results from the OLS Regression Results with `summary` method\n",
    "- SSR the Sum of the Squared Residuals is the difference in error from the regression line (try to minimize)\n",
    "- SST the Sum of the Squared Total is the difference to the target mean\n",
    "- SSE the Sum of the Squared Error is the variation of the regression line to the mean\n",
    "- RMSE Root Mean Squared Error is the square root of the SSE\n",
    "- $R^2$ is the fraction of the variance of the original data explained by the model, want a number close to 1.\n",
    "\n",
    "We created a function to scale data reconstructing the mean standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Your Turn</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
