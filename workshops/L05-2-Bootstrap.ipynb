{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"instructions\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h1>Instructions</h1>\n",
    "<p>Look for the <b>2 Your Turn</b> sections to complete the code and/or answer questions.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 - Bootstrap and Resample Methods\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Bootstrap and resampling are widely applicable statistical methods which relax many of the assumptions of classical statistics. Resampling methods draw heavily on the CLT. Specifically resampling methods:\n",
    "\n",
    "- Allow computation of statistics from limited data\n",
    "- Compute statistics from multiple subsamples of dataset\n",
    "- Minimal distribution assumptions\n",
    "- Computationally intensive\n",
    "\n",
    "Commonly used resampling methods include:\n",
    "\n",
    "- Randomization or permutation methods (e.g., Fisher's exact test)\n",
    "- Bootstrap: resample with equivalent size and replacement\n",
    "- Jackknife: leave one out resampling\n",
    "- Cross validation: resample into folds without replacement\n",
    "\n",
    "\n",
    "## Pitfalls\n",
    "\n",
    "But, there is no magic involved. When using resampling methods always keep in mind the several pitfalls:\n",
    "- If sample is biased, the resample statistic is biased.\n",
    "- Sample variance and CIs are no better than what the sample allows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "### Randomization and permutation methods\n",
    "\n",
    "Randomization and permutation methods were pioneered by Fisher as early as 1911. Fisher fully developed the theory in his 1935 book. Scalability of these methods remain limited, even with modern computers. \n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "Cross-validation was originally proposed by Kurtz in 1948. Mosier extended the method to double cross validation in 1951. The modern method of nested or multicross-validation were introduced by Krus and Fuller in 1982. Today, cross-validation is widely used in the testing of machine learning models. \n",
    "\n",
    "### Jackknife Methods\n",
    "\n",
    "Maurice Quenouille originally suggested this method in 1949. The jackknife was fully developed\n",
    "by John W. Tukey, who gave the method its name, in 1958. Tukey saw that method as a simple tool useful for many purposes like a pocket knife. \n",
    "\n",
    "\n",
    "### Bootstrap \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/Efron1980.jpg\" style=\"float: left; height: 150px; padding-left: 10px;\" title=\"Efron's 1980 book.\" />\n",
    "The bootstrap method was first suggested by Efron and Hinkley in 1978 and further developed by Efron in 1979. A full treatment was provided in Efron's 1980 book. \n",
    "\n",
    "With increased computing power, use of bootstrap methods continues to expand. Check out <a title=\"YouTube: Interview with Bradley Efron, BBVA Foundation Frontiers of Knowledge Award in Basic Sciences\" href=\"https://www.youtube.com/embed/H2tOhMaXWvI\" target=\"newwindow\">Efron himself explaining</a> the usefulness of bootstrap method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Bootstrap\n",
    "\n",
    "The bootstrap method:\n",
    "\n",
    "- Re-compute statistic many times with sample \n",
    "- Randomly sample (e.g., Bernoulli sample) data with replacement\n",
    "- Subsamples have the same size as original sample\n",
    "- Works with any statistic â€¦ in principle\n",
    "\n",
    "For example, you can compute the bootstrap mean as:\n",
    "\n",
    "$$Meanboot = \\frac{\\Sigma_i mean(sample_i)}{nsample}\\\\\n",
    "where,\\ for\\ example\\ with\\ 10\\ samples,\\\\\n",
    "sample_i = X_1 + X_2 + X_3 + X_4 + X_5 + X_6 + X_7 + X_8 + X_1 + X_5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Example\n",
    "\n",
    "Let's try a first example. You will bootstrap the means of the heights of different populations from Galton's height data. This is an example of a parametric bootstrap estimate. Parametric because our model has a parameter, the mean, we are trying to estimate. \n",
    "\n",
    "As a first step, the code in the cell below divides the data set by male and female adult children, and then plots the distributions with the means of the two populations. Run this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import seaborn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "families = pandas.read_csv('https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/GaltonFamilies.csv', \n",
    "                            index_col=0)\n",
    "families.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select a subset of rows from a dataframe that match a given condition like so (the condition is gender in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male = families[families.gender == 'male']\n",
    "female = families[families.gender == 'female']\n",
    "\n",
    "len(male), len(female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can get the min or max of a series of values within a \n",
    "dataframe like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "families.childHeight.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hist(x, p=5):\n",
    "    # Plot the distribution and mark the mean\n",
    "    pyplot.hist(x, alpha=.5)\n",
    "    pyplot.axvline(x.mean())\n",
    "    # 95% confidence interval    \n",
    "    pyplot.axvline(numpy.percentile(x, p/2.), color='red', linewidth=3)\n",
    "    pyplot.axvline(numpy.percentile(x, 100-p/2.), color='red', linewidth=3)\n",
    "    \n",
    "def plot_dists(a, b, nbins, a_label='pop_A', b_label='pop_B', p=5):\n",
    "    # Create a single sequence of bins to be shared across both\n",
    "    # distribution plots for visualization consistency.\n",
    "    combined = pandas.concat([a, b])\n",
    "    breaks = numpy.linspace(\n",
    "        combined.min(), \n",
    "        combined.max(), \n",
    "        num=nbins+1)\n",
    "\n",
    "    pyplot.subplot(2, 1, 1)\n",
    "    plot_hist(a)\n",
    "    pyplot.title(a_label)\n",
    "    \n",
    "    pyplot.subplot(2, 1, 2)\n",
    "    plot_hist(b)\n",
    "    pyplot.title(b_label)\n",
    "    \n",
    "    pyplot.tight_layout()\n",
    "    \n",
    "\n",
    "plot_dists(male.childHeight, female.childHeight, 20, a_label='sons', b_label='daughters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the distributions of the heights of the sons and daughters overlap. But, are the means significantly different?\n",
    "\n",
    "### Bootstrap the Means\n",
    "\n",
    "Pandas has built-in support for generating bootstrap samples from a given dataframe. We'll use the `.sample()` method of our two dataframes to draw a single bootstrap sample like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female.sample(frac=1, replace=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the mean of our bootstrap sample in the same way as our full dataset dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female.sample(frac=1, replace=True).childHeight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll take many (`n_replicas`) bootstrap samples and plot the distribution of sample means as well as the mean of the sample means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_replicas = 1000\n",
    "\n",
    "female_bootstrap_means = pandas.Series([\n",
    "    female.sample(frac=1, replace=True).childHeight.mean()\n",
    "    for i in range(n_replicas)])\n",
    "\n",
    "male_bootstrap_means = pandas.Series([\n",
    "        male.sample(frac=1, replace=True).childHeight.mean()\n",
    "        for i in range(n_replicas)])\n",
    "\n",
    "plot_dists(male_bootstrap_means, female_bootstrap_means, \n",
    "           nbins=80, a_label='sons', b_label='daughters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the bootstrap means do not overlap at all. Evidently, the **difference is significant**. \n",
    "\n",
    "## Your Turn 1\n",
    "Compute and plot the bootstrap distribution of the mean of the heights of the fathers for male children. Compare this estimate to the bootstrap estimate of the mean of the heights of the sons (already computed). Are these means significantly different? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace the ???\n",
    "father_bootstrap_means = pandas.Series([\n",
    "    ???.sample(frac=1, replace=True).???.mean()\n",
    "    for i in range(n_replicas)])\n",
    "\n",
    "#Replace the ???\n",
    "plot_dists(???, ???, \n",
    "           nbins=80, a_label='???', b_label='???')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Difference in Means\n",
    "\n",
    "In the previous example you computed and compared two bootstrap distributions of the means from the height data. It is actually quite easy to bootstrap a statistic such as the difference in means. \n",
    "\n",
    "In the cell below, we generate bootstrap samples from the full male+female dataset and then difference in the means of male and female `childHeight` for each sample, generating a distribution of sample mean `diffs`. \n",
    "\n",
    "Run the code and examine the plotted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for i in range(n_replicas):\n",
    "    sample = families.sample(frac=1.0, replace=True)\n",
    "    male_sample_mean = sample[sample.gender == 'male'].childHeight.mean()\n",
    "    female_sample_mean = sample[sample.gender == 'female'].childHeight.mean()\n",
    "    diffs.append(male_sample_mean - female_sample_mean)\n",
    "diffs = pandas.Series(diffs)\n",
    "\n",
    "plot_hist(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the **difference in means is far from zero**. As before, we can infer that the means of the two populations are **significantly different**. \n",
    "\n",
    "But, is the distribution of the difference in means normal, as implied by the CLT? Run the code in the cell below to create a Q-Q normal plot of the bootstrap distribution of the difference in means and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Is the bootstrapped distribution Normal?\n",
    "fig = sm.qqplot(diffs, line='s')\n",
    "pyplot.title('Quantiles of standard Normal vs. bookstrapped mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points on the Q-Q Normal plot are nearly on a strait line. Apparently, the bootstrap distribution of the difference in means does conform to the CLT.\n",
    "\n",
    "## Your Turn 2\n",
    "In the cell below, create and run the code to compare the difference in means of adult sons to their fathers. Is the difference significant? Also, check if the distribution in the difference in means is approximately normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the ???\n",
    "diffs = []\n",
    "for i in range(n_replicas):\n",
    "    sample = families.sample(frac=1.0, replace=True)\n",
    "    male_sample_mean = sample[sample.gender == '???'].???.mean()\n",
    "    father_sample_mean = sample[sample.gender == '???'].???.mean()\n",
    "    diffs.append(male_sample_mean - father_sample_mean)\n",
    "diffs = pandas.Series(diffs)\n",
    "\n",
    "plot_hist(diffs)\n",
    "\n",
    "fig = sm.qqplot(diffs, line='s')\n",
    "pyplot.title('Quantiles of standard Normal vs. bootstrapped mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstraping Other Statistics\n",
    "\n",
    "The bootstrap method can be applied to most any statistic. The code in the cell below computes the difference in **medians** of the sons and the fathers. The median is an order statistic, and the values of a median are discretized by the quanta of the samples. Run this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "median_diffs = []\n",
    "for i in range(n_replicas):\n",
    "    sample = families.sample(frac=1.0, replace=True)\n",
    "    male_sample_median = sample[sample.gender == 'male'].childHeight.median()\n",
    "    father_sample_median = sample[sample.gender == 'male'].father.median()\n",
    "    median_diffs.append(male_sample_median - father_sample_median)\n",
    "median_diffs = pandas.Series(median_diffs)\n",
    "\n",
    "plot_hist(median_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows that the bootstrap distribution of the difference in medians. The **confidence interval overlaps zero**, so we must accept the null hypothesis that the difference in medians is zero.\n",
    "\n",
    "You can also plot the Q-Q Normal plot of the bootstrap distribution of the difference in medians. Run the code in the cell below and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Is the bootstrapped distribution Normal?\n",
    "fig = sm.qqplot(median_diffs, line='s')\n",
    "pyplot.title('Quantiles of standard Normal vs. bootstrapped median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the **CLT only applies to sample means**, not medians or any other statistic. None the less, you can see that the Q-Q normal plot of the bootstrap distribution of the difference in medians tends down the center diagonal of the plot. The zig-zag behavior arises from the quantization inherent in the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Jackknife\n",
    "\n",
    "The jackknife is another resampling method. The jackknife is related to the bootstrap, but is a bit more restrictive in its application. Specifically, the jackknife:\n",
    "\n",
    "- Re-computes a statistic N times with leave-one-out sample, where N is the sample size.\n",
    "- Uses Random leave one (or n) out sampling.\n",
    "- Only works for statistics with continuous derivatives.\n",
    "\n",
    "For example, you can compute the bootstrap mean as:\n",
    "\n",
    "$$MeanJK = \\frac{\\Sigma_i mean(sample_i)}{nsample}\\\\\n",
    "where,\\ for\\ example\\ with\\ 10\\ samples,\\\\\n",
    "sample_i = X_1 + X_2 + X_3 + X_4 + X_5 + X_6 + X_8 + X_9 + X_{10}$$\n",
    "\n",
    "A simple example of using a jackknife estimation of a mean is shown in the cell below. Run this code, examine the results and compare them to those obtained with the bootstrap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"We have %d male rows and %d female rows\" % (len(male), len(female)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the index of each row within a set (either `male` or `female`) to leave each of the observations (rows) out, one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-number the rows in each set starting from zero or else the row indices\n",
    "# would still match the original `families` dataframe we started with.\n",
    "male = male.reset_index(drop=True)\n",
    "female = female.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop any particular row(s) we like and compute our jackknife sample mean on the remaining rows.\n",
    "\n",
    "One trick to selecting all rows except for a given row index is the following, where we drop the row at index==3 (note the left-most column values skip 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male.drop(male.iloc[[3]].index).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute a summary statistic of our choice on the rows that were not dropped like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male.drop(male.iloc[[3]].index).childHeight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this strategy to leave each of the rows out, one at a time, and collect the summary statistics (child height mean) for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male_jk_means = pandas.Series([\n",
    "        male.drop(male.iloc[[i]].index).childHeight.mean()\n",
    "        for i in range(len(male))])\n",
    "\n",
    "male_jk_means.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the gender==female subset of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_jk_means = pandas.Series([\n",
    "        female.drop(male.iloc[[i]].index).childHeight.mean()\n",
    "        for i in range(len(female))])\n",
    "\n",
    "female_jk_means.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Cross validation is a leave-many-out resampling technique. Cross validation uses multiple non-overlapping subsamples to train and test a data set. Basic cross validation uses the following steps:\n",
    "\n",
    "- Divide dataset into N subsamples \n",
    "- N â€“ 1 Folds train model \n",
    "- One Fold evaluate model\n",
    "\n",
    "The basic concept of cross validation is illustrated below. \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/CrossValidation.jpg\" style=\"height: 200px;\" title=\"Cross validation illustrated\" />\n",
    "\n",
    "Model performance is performed by looking at aggregated summary statistics across the the trained models using the test data for each resample. The resampling reduces bias in the model performance statistics. Most importantly, cross validation reduces unpleasant surprises when a model is placed in production. \n",
    "\n",
    "The jackknife is an extreme end member of the family of cross validation methods. With the jackknife, the number of folds equals the number of data points. \n",
    "\n",
    "There are several variations on cross validation. The most commonly used in **nest cross validation**. In nested cross validation, cross validation is performed on one or more models to compare performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Your Turn</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
