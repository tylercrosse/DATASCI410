{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"instructions\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h1>Instructions</h1>\n",
    "<p>Look for the <b>2 Your Turn</b> sections to complete the code and/or answer questions.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 - Linear Regression and Bootstrapping\n",
    "\n",
    "---------------------------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "----------\n",
    "Bootstrapping is a procedure that enables us to determine the variability in a dataset or parameters in a model and can work with small amounts of data.\n",
    "\n",
    "In fact, the origin of the name is based in literature. Since this procedure appears to gain insight from very little information, one considers that we may be 'pulling ourselves up by our bootstraps'.\n",
    "\n",
    "> \"There were others who had forced their way to the top from the lowest rung by the aid of their bootstraps.\"\n",
    "\n",
    "- James Joyce, 'Ulysses' 1922\n",
    "\n",
    "Idea:\n",
    "\n",
    "We treat the data sample we have as the only representation of the population that we have. Then to get more datasets from it, we resample the data _with replacement_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we polled a classroom of 15 students on how old the teacher is.  We would get 15 answers, illustrated below.\n",
    "\n",
    "For this section we will use a library called `scikits.bootstrap` that may be installed via `$ pip install sckikits.bootstrap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this install command if you do not have this package. Uncomment and run.\n",
    "# !pip install scikits.bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import scikits.bootstrap as bootstrap\n",
    "\n",
    "# Student guesses of the teacher's age. (Notice the last entry is an outlier!)\n",
    "teacher_age = [27, 33, 31, 33, 35, 29, 39, 41, 36, 34, 35, 29, 30, 35, 97]\n",
    "\n",
    "# Expected age could be the average\n",
    "teacher_avg = np.mean(teacher_age)\n",
    "print('Average of teacher age guesses: {} years old.'.format(teacher_avg))\n",
    "\n",
    "# Let's look at the standard deviation as well:\n",
    "teacher_std = np.std(teacher_age)\n",
    "print('St.Dev. of teacher age guesses: {} years.'.format(teacher_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the guesses, only 3 of them are above the average. We see that this is because of one outlier of 97 years.  While we may consider dropping this outlier to get a better guess, we can also use bootstrapping. Here we will repeatedly sample _with replacement_ to get a set of samples. Each sample will also have 15 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_sets = 100\n",
    "n_samples = len(teacher_age)\n",
    "\n",
    "def generate_sample(dataset, n):\n",
    "    return list(np.random.choice(dataset, size=n, replace=True))\n",
    "\n",
    "boot_samples = [generate_sample(teacher_age, n_samples) for _ in range(n_sets)]\n",
    "\n",
    "print('Here are the top 3 samples generated:')\n",
    "print('{}, ...'.format(boot_samples[0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the mean and standard deviations of each of the 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_means = [np.mean(x) for x in boot_samples]\n",
    "sample_stdev = [np.std(x) for x in boot_samples]\n",
    "\n",
    "# Now take the average of all the means\n",
    "set_mean = np.mean(sample_means)\n",
    "# And the average of all the st. devs\n",
    "set_stdev = np.mean(sample_stdev)\n",
    "\n",
    "print('Average of the sample averages: {}'.format(set_mean))\n",
    "print('Average of the sample st. devs: {}'.format(set_stdev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimmed Mean Function\n",
    "So far, this bootstrapping hasn't really helped us much.  The sample averages of the sets are very similar (as we would expect).  BUT we also haven't done much with the sample sets we created.\n",
    "\n",
    "Let us consider a function called 'trimmed mean' which will drop the lowest and the highest N elements from a set of numbers.\n",
    "\n",
    "We will apply this function to each sample and _then_ consider the averages and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a trimmed_mean function\n",
    "\n",
    "def trimmed_mean(dataset, n):\n",
    "    # Sort dataset in ascending order\n",
    "    sorted_list = sorted(dataset)\n",
    "    # Trim the list\n",
    "    trimmed_list = sorted_list[n:(len(sorted_list) - n)]\n",
    "    return(trimmed_list)\n",
    "\n",
    "# Apply trimmed mean to all samples\n",
    "boot_trims = [trimmed_mean(x, 1) for x in boot_samples]\n",
    "\n",
    "# Get each sample mean and st. dev.\n",
    "trimmed_means = [np.mean(x) for x in boot_trims]\n",
    "trimmed_stdevs = [np.std(x) for x in boot_trims]\n",
    "\n",
    "# Now take the average of all the means\n",
    "trimmed_mean_avg = np.mean(trimmed_means)\n",
    "# And the average of all the standard deviations\n",
    "trimmed_stdev = np.mean(trimmed_stdevs)\n",
    "\n",
    "print('Average of the sample averages: {}'.format(trimmed_mean_avg))\n",
    "print('Average of the sample st. devs: {}'.format(trimmed_stdev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the change in numbers!!  \n",
    "\n",
    "### Histogram of the Means\n",
    "Let's try a histogram of the means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "bins = [x for x in range(25, 55)]\n",
    "\n",
    "ax = sns.distplot(sample_means, bins, label='Orig. Bootstrap')\n",
    "ax = sns.distplot(trimmed_means, bins, label='Trimmed')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have less outlier data to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn 1\n",
    "\n",
    "### Question:\n",
    "------------------\n",
    "\n",
    " - Why do you think the above distributions are slightly multi-modal? (i.e., have multiple peaks?)\n",
    " \n",
    "------------------\n",
    "\n",
    "If you are unsure as to why there are multiple peaks to the right, try looking at individual bootstrap sets that happen to have very large means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming More\n",
    "Now let's look at what happens when we trim more than one point off the top and bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the different trimmed means\n",
    "boot_trim1 = [trimmed_mean(x, 1) for x in boot_samples]\n",
    "boot_trim2 = [trimmed_mean(x, 2) for x in boot_samples]\n",
    "boot_trim3 = [trimmed_mean(x, 3) for x in boot_samples]\n",
    "boot_trim4 = [trimmed_mean(x, 4) for x in boot_samples]\n",
    "\n",
    "# Get each sample mean and st. dev.\n",
    "trimmed_mean1 = [np.mean(x) for x in boot_trim1]\n",
    "trimmed_stdev1 = [np.std(x) for x in boot_trim1]\n",
    "\n",
    "trimmed_mean2 = [np.mean(x) for x in boot_trim2]\n",
    "trimmed_stdev2 = [np.std(x) for x in boot_trim2]\n",
    "\n",
    "trimmed_mean3 = [np.mean(x) for x in boot_trim3]\n",
    "trimmed_stdev3 = [np.std(x) for x in boot_trim3]\n",
    "\n",
    "trimmed_mean4 = [np.mean(x) for x in boot_trim4]\n",
    "trimmed_stdev4 = [np.std(x) for x in boot_trim4]\n",
    "\n",
    "# Plot the means\n",
    "bins = [x for x in range(25, 55)]\n",
    "\n",
    "ax = sns.distplot(trimmed_mean1, bins, label='Trimmed 1')\n",
    "ax = sns.distplot(trimmed_mean2, bins, label='Trimmed 2')\n",
    "ax = sns.distplot(trimmed_mean3, bins, label='Trimmed 3')\n",
    "ax = sns.distplot(trimmed_mean4, bins, label='Trimmed 4')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be hard to see, but notice that we get less and less multi-modality as we trim more and more off from the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Bootstrapping Regression\n",
    "\n",
    "The bootstrap method can be applied to regression models. Bootstrapping a regression model gives insight on how variable the model parameters are. It is useful to know how much random variation there is in regression coefficients simply because of small changes in data values. \n",
    "\n",
    "As with most statistics, it is possible to bootstrap most any regression model. However, since bootstrap resampling uses a large number of subsamples, it can be computationally intensive. For large-scale problems it is necessary to using other resampling methods like cross-validation.\n",
    "\n",
    "First we setup our data and fit the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# Create sample data\n",
    "n_points = 25\n",
    "x = np.linspace(0, 10, n_points)\n",
    "y = x + (np.random.rand(len(x)) * 5)\n",
    "data_df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=data_df)\n",
    "\n",
    "results = ols_model.fit()\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, x-Slope : {}'.format(results.params))\n",
    "\n",
    "y_pred = ols_model.fit().predict(x)\n",
    "# Plot result\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample Points (Parametric Bootstrap)\n",
    "\n",
    "The first type of bootstrapping we will perform is resampling points. We'll use the pandas function `sample` to help with this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resample with replacement each row\n",
    "boot_slopes = []\n",
    "boot_interc = []\n",
    "n_boots = 100\n",
    "plt.figure()\n",
    "for _ in range(n_boots):\n",
    "    # Create a sampling of the rows, same size, with replacement\n",
    "    sample_df = data_df.sample(n=n_points, replace=True)\n",
    "    # Fit a linear regression\n",
    "    ols_model_temp = sm.ols(formula = 'y ~ x', data=sample_df)\n",
    "    results_temp = ols_model_temp.fit()\n",
    "    \n",
    "    # Get the intercept and slope, and record it\n",
    "    boot_interc.append(results_temp.params[0])\n",
    "    boot_slopes.append(results_temp.params[1])\n",
    "    \n",
    "    # Plot a greyed out line\n",
    "    y_pred_temp = ols_model_temp.fit().predict(sample_df['x'])\n",
    "    plt.plot(sample_df['x'], y_pred_temp, color='grey', alpha=0.2)\n",
    "\n",
    "# Add points\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shape of the greyed bands.  They are more concentrated at the center than the edge.  This is because more points are positioned around the center than the edges.\n",
    "\n",
    "Let's look at the distributions of the slope and intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(boot_slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(boot_interc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these above plots and sets of data it is easy to determine a confidence interval for our linear regression parameters if need be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn 2\n",
    "\n",
    "### Question:\n",
    "\n",
    "From the above sets of 100 slopes and 100 intercepts, what would be a good 90% confidence interval for our slope and intercept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hint: think of sorting the slopes and intercepts first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our data is very nice here. If by chance we had sparse data, there may be a chance that our random selection of points are entirely in one area and not in another. To combat this, we can apply a different type of bootstrapping, called 'Non-Parametric Bootstrapping' in which we will apply bootstrapping on the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample Residuals (Non-Parametric Bootstrapping)\n",
    "\n",
    "The idea is that we want to observe a point at every x-position that we have a point in the original data. So we fit the original data and re-shuffle the residuals at every point. This should be fine because we are assuming that the errors are IID among our points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "n_points = 25\n",
    "x = np.linspace(0, 10, n_points)\n",
    "y = x + (np.random.rand(len(x)) * 5)\n",
    "data_df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "# Fit linear regression model\n",
    "ols_model = sm.ols(formula = 'y ~ x', data=data_df)\n",
    "results = ols_model.fit()\n",
    "\n",
    "# Get predicted values\n",
    "y_pred = results.predict(x)\n",
    "\n",
    "# Get residual values\n",
    "resids = results.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the fitted values and residuals from our fit, we will generate N (100) new sets of data by sampling the residuals and adding them to the fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boot2_slopes = []\n",
    "boot2_interc = []\n",
    "n_boots = 100\n",
    "plt.figure()\n",
    "for _ in range(n_boots):\n",
    "    # Create a sampling of the residuals with replacement\n",
    "    boot_resids = np.random.choice(resids, n_points, replace=True)\n",
    "    y_temp = [y_pred_i + resid_i for y_pred_i, resid_i in zip(y_pred, boot_resids)]\n",
    "    \n",
    "    sample_df = pd.DataFrame({'x': list(x), 'y': y_temp})\n",
    "    # Fit a linear regression\n",
    "    ols_model_temp = sm.ols(formula = 'y ~ x', data=sample_df)\n",
    "    results_temp = ols_model_temp.fit()\n",
    "    \n",
    "    # Get the intercept and slope, and record it\n",
    "    boot2_interc.append(results_temp.params[0])\n",
    "    boot2_slopes.append(results_temp.params[1])\n",
    "    \n",
    "    # Plot a greyed out line\n",
    "    y_pred_temp = ols_model_temp.fit().predict(sample_df['x'])\n",
    "    plt.plot(sample_df['x'], y_pred_temp, color='grey', alpha=0.2)\n",
    "\n",
    "# Add points\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred, linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look again at the distributions of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(boot2_slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(boot2_interc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, this gives us a handle on the error in our parameters with linear regression. In fact, this type of bootstrapping can be extended to most linear and non-linear models that have a unique optimal solution. This allows us to get a distribution of error for every parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we looked at applying bootstrapping techniques to linear regression in two ways:\n",
    "\n",
    "**Parametric bootstrapping** - resampling from all of the points:\n",
    "1. Sample the data with replacement numerous times (100).\n",
    "2. Fit a linear regression to each sample.\n",
    "3. Store the intercept and slopes.\n",
    "4. Plot a histogram of the parameters.\n",
    "\n",
    "\n",
    "**Non-parametric boostrapping** - resampling on the residuals with an uneven distribution of feature values:\n",
    "1. Find the optimal linear regression on all the original data.\n",
    "2. Extract the residuals from the fit.\n",
    "3. Create new y values using the residual samples.\n",
    "4. Fit the linear regression with the new y values.\n",
    "5. Store the slope and intercepts.\n",
    "6. Plot a histogram of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Your Turn</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
